{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Ben Caruso\n",
    "# Pipeline to read in word documents\n",
    "\n",
    "import docx\n",
    "\n",
    "import os\n",
    "import subprocess"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Functionality to read in docs, convert to docx, and then read them all into jupyter"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Step 1: Conversion of documents to .docx extension**\n",
    "\n",
    "Initially, the documents I was given were audio recordings as .doc files in Word. Upon trying to read them in using the python-docx library I chose, I found that I was not able to as they weren't considered readable. Thus, I needed to convert each to a readable .docx file before reading them in. So, I used the subprocess module to automate the process, rather than convert all by hand. The function I wrote below checks the current directory for all .doc files, and runs a textutil (built-in functionality on mac that works with file conversion) command to copy the document as a .docx file. In order to work, the current directory must be set to whichever directory contains the .doc objects you want to convert. Change it with os.chdir(dir_name) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Change current directory to desired directory\n",
    "#os.chdir(dir_name)\n",
    "\n",
    "# Script to convert docs within a given directory to .docx - makes a copy for each doc\n",
    "def convert_doc_to_docx():\n",
    "    # os.listdir defaults to the current directory\n",
    "    for filename in os.listdir():\n",
    "        if filename.endswith('.doc'):\n",
    "            # Creates a copy with the .docx extension\n",
    "            # Works on mac because textutil is built in\n",
    "            # soffice if using windows\n",
    "            subprocess.call(['textutil', '-convert', 'docx', filename])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Step 2: Read in documents with .docx extension**\n",
    "\n",
    "This function reads all .docx files from the current directory using the python-docx module, and calls my docx_to_text() function below to convert it to a block of text. It assumes consistent formatting between all the .docx files passed in, which is important. The docx_to_text() function was written to handle the format of the interviews I've been working with, and will only work with this exact format, which suffices for this problem but may not for others. Again, in order for this function to work properly, the current directory must contain the .docx files of interest."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def docx_to_text(docx):\n",
    "    '''\n",
    "    This function will read in a word document (docx) utilizing the python-docx library\n",
    "    \n",
    "    Input\n",
    "        docx (docx.Document object): Word document as a .docx\n",
    "    \n",
    "    Returns\n",
    "        Interviewer (string): Block of text consisting of interviewer's statements\n",
    "        Farmer (string): Block of text consisting of farmer's responses and statements\n",
    "    '''\n",
    "    \n",
    "    # Convert document into paragraphs\n",
    "    paragraphs = docx.paragraphs\n",
    "    \n",
    "    # Convert paragraphs into conversation\n",
    "    convo = []\n",
    "    for para in paragraphs:\n",
    "        if para.text != '':\n",
    "            convo.append(para.text)\n",
    "    \n",
    "    # Split into interviewer and interviewee lists\n",
    "    interviewer = []\n",
    "    farmer = []\n",
    "\n",
    "    for para in convo:\n",
    "        if 'Interviewer' in para or 'Meredith' in para:\n",
    "            interviewer.append(para)\n",
    "        else:\n",
    "            farmer.append(para)\n",
    "            \n",
    "    # Process into cleaner blocks of text\n",
    "    clean_interviewer = \"\"\n",
    "    for text in interviewer:\n",
    "        if '\\t' in text:\n",
    "            clean_interviewer += (text.split('\\t')[1]) + \" \"\n",
    "        else:\n",
    "            clean_interviewer += text + \" \"\n",
    "            \n",
    "    clean_farmer = \"\"\n",
    "    for text in farmer:\n",
    "        if '\\t' in text:\n",
    "            clean_farmer += text.split('\\t')[1] + \" \"\n",
    "        else:\n",
    "            clean_farmer += text + \" \"\n",
    "        \n",
    "    # preprocessing to clean up words and text so that text is lowercase\n",
    "    clean_interviewer = clean_interviewer.lower()\n",
    "    clean_farmer = clean_farmer.lower()\n",
    "        \n",
    "    return clean_interviewer, clean_farmer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def read_docx(docx):\n",
    "    '''\n",
    "    This function will read in a word document (docx) utilizing the python-docx library and return a\n",
    "    list of Q & A tuples containing an interviewer's question/statement and the interviewee's direct response\n",
    "    \n",
    "    Input: \n",
    "        docx : docx.Document object - Word document as a .docx\n",
    "    \n",
    "    Output:\n",
    "        Interviewer (list): List containing tuples that each contain an index and\n",
    "                                        text consisting of interviewer's statements\n",
    "        Farmer (list): List containing tuples that each contain an index and\n",
    "                                        text consisting of farmers's responses and statements\n",
    "    '''\n",
    "    \n",
    "    # Convert document into paragraphs\n",
    "    paragraphs = docx.paragraphs\n",
    "    \n",
    "    # Convert paragraphs into conversation\n",
    "    convo = []\n",
    "    for para in paragraphs:\n",
    "        if para.text != '':\n",
    "            convo.append(para.text)\n",
    "    \n",
    "    # Split into interviewer and interviewee lists\n",
    "    interviewer = []\n",
    "    interviewee = []\n",
    "\n",
    "    for index, para in enumerate(convo):\n",
    "        if 'Interviewer' in para or 'Meredith' in para:\n",
    "            interviewer.append([index, para])\n",
    "        else:\n",
    "            interviewee.append([index-1, para])\n",
    "            \n",
    "    # Process into cleaner lists\n",
    "    clean_interviewer = []\n",
    "    for index, text in interviewer:\n",
    "        if '\\t' in text:\n",
    "            clean_interviewer.append((int(index / 2), text.split('\\t')[1]))\n",
    "    \n",
    "    clean_interviewee = []\n",
    "    for index, text in interviewee:\n",
    "        if '\\t' in text:\n",
    "            clean_interviewee.append((int(index / 2), text.split('\\t')[1]))\n",
    "    \n",
    "    return clean_interviewer, clean_interviewee"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function to read in all docx files from the current directory\n",
    "# Make sure you are in the correct directory before proceeding with this function\n",
    "def read_in_docx():\n",
    "    \n",
    "    # Define list of doc_texts\n",
    "    doc_texts = []\n",
    "    \n",
    "    # Searches current directory for .docx files\n",
    "    for filename in os.listdir():\n",
    "        if filename.endswith('.docx'):\n",
    "            # Convert to doc object and append to list\n",
    "            doc = docx.Document(filename)\n",
    "            \n",
    "            # Convert doc object to a block of text - ASSUMES CONSISTENT FORMATTING\n",
    "            doc_texts.append(docx_to_text(doc))\n",
    "            \n",
    "            \n",
    "    return doc_texts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "os.chdir('Vermont Transcripts/farmer_interviews')\n",
    "vt_farmer_texts = read_in_docx()\n",
    "# Worked on vermont transcripts/farmer interviews\n",
    "\n",
    "# Returned a list of tuples that consist of the entire interviewer text and farmer text\n",
    "\n",
    "os.chdir('../../')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "os.chdir('Vermont Transcripts/expert_interviews/')\n",
    "vt_expert_texts = read_in_docx()\n",
    "os.chdir('../../')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "os.chdir('UMaine Transcripts/farmer_interviews')\n",
    "me_farmer_texts = read_in_docx()\n",
    "os.chdir('../../')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "os.chdir('UMaine Transcripts/expert_interviews')\n",
    "me_expert_texts = read_in_docx()\n",
    "os.chdir('../../')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Step 3: Convert to texts**\n",
    "\n",
    "Next, I convert list of tuples returned by read_in_docx using a simple text_process function and return a large string containing only the interviewer's response"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "def text_process(text_param):\n",
    "    # Takes a list of tuples and returns a string containing only the interviewer's responses\n",
    "    text_list = [text[1] for text in text_param]\n",
    "    \n",
    "    text_str = \"\"\n",
    "    \n",
    "    return text_str.join(text_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "vt_farmer_text = text_process(vt_farmer_texts)\n",
    "vt_expert_text = text_process(vt_expert_texts)\n",
    "me_farmer_text = text_process(me_farmer_texts)\n",
    "me_expert_text = text_process(me_expert_texts)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'sure. yes, i’m hannah doyle, i have boneyard farm here, and it is 10 acres total, though a lot of th'"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# This is a collection of unprocessed (except for converted to lowercase) text of the vermont farmers' interview\n",
    "# responses. This is the raw data that will be processed using spacy's nlp() function\n",
    "vt_farmer_text[:100]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'cool. i didn’t prepare anything. i mean, i just –i made a couple notes this morning. okay, good. yea'"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# This is a collection of unprocessed (except for converted to lowercase) text of the vermont experts' interview\n",
    "# responses. This is the raw data that will be processed using spacy's nlp() function\n",
    "vt_expert_text[:100]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Read the texts out to .txt files so that I could perform preprocessing on the actual text data itself in another script"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Read out text strings to a text file\n",
    "vt_farmer_file = open(\"vt_farmer_text.txt\", \"w\")\n",
    "vt_expert_file = open(\"vt_expert_text.txt\", \"w\")\n",
    "me_farmer_file = open(\"me_farmer_text.txt\", \"w\")\n",
    "me_expert_file = open(\"me_expert_text.txt\", \"w\")\n",
    "\n",
    "vt_farmer_file.write(vt_farmer_text)\n",
    "vt_expert_file.write(vt_expert_text)\n",
    "me_farmer_file.write(me_farmer_text)\n",
    "me_expert_file.write(me_expert_text)\n",
    "\n",
    "vt_farmer_file.close()\n",
    "vt_expert_file.close()\n",
    "me_farmer_file.close()\n",
    "me_expert_file.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 189,
   "metadata": {},
   "outputs": [],
   "source": [
    "def read_in_docs_as_qna():\n",
    "    # Define list of doc_texts\n",
    "    doc_texts = []\n",
    "\n",
    "    # Searches current directory for .docx files\n",
    "    for filename in os.listdir():\n",
    "        if filename.endswith('.docx'):\n",
    "\n",
    "            # Convert to doc object and append to list\n",
    "            doc = docx.Document(filename)\n",
    "\n",
    "            # Convert doc object to Q&A tuples - ASSUMES CONSISTENT FORMATTING\n",
    "            doc_texts.append(read_docx(doc))\n",
    "    \n",
    "    return doc_texts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 197,
   "metadata": {},
   "outputs": [],
   "source": [
    "def read_files(qna = True):\n",
    "    \"\"\"Reads in all files at once. Returns 4 items\n",
    "       Run from home directory with pathnames double-checked\"\"\"\n",
    "    \n",
    "    os.chdir('Vermont Transcripts/farmer_interviews')\n",
    "    if qna:\n",
    "        vt_farmer_texts = read_in_docs_as_qna()\n",
    "    else:\n",
    "        vt_farmer_texts = read_in_docx()\n",
    "    os.chdir('../../')\n",
    "    \n",
    "    os.chdir('Vermont Transcripts/expert_interviews/')\n",
    "    if qna:\n",
    "        vt_expert_texts = read_in_docs_as_qna()\n",
    "    else:\n",
    "        vt_expert_texts = read_in_docx()\n",
    "    os.chdir('../../')\n",
    "\n",
    "    os.chdir('UMaine Transcripts/farmer_interviews/')\n",
    "    if qna:\n",
    "        me_farmer_texts = read_in_docs_as_qna()\n",
    "    else:\n",
    "        me_farmer_texts = read_in_docx()\n",
    "    os.chdir('../../')\n",
    "\n",
    "    os.chdir('UMaine Transcripts/expert_interviews/')\n",
    "    if qna:\n",
    "        me_expert_texts = read_in_docs_as_qna()\n",
    "    else:\n",
    "        me_expert_texts = read_in_docx()\n",
    "    os.chdir('../../')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
